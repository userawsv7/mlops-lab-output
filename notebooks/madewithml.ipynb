{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acbetMKBt825"
      },
      "source": [
        "<div align=\"center\">\n",
        "<h1><img width=\"30\" src=\"https://madewithml.com/static/images/rounded_logo.png\">&nbsp;<a href=\"https://madewithml.com/\">Made With ML</a></h1>\n",
        "    <h3>ML for Developers</h3>\n",
        "    Design · Develop · Deploy · Iterate\n",
        "</div>\n",
        "\n",
        "<br>\n",
        "\n",
        "<div align=\"center\">\n",
        "    <a target=\"_blank\" href=\"https://madewithml.com\"><img src=\"https://img.shields.io/badge/Subscribe-40K-brightgreen\"></a>&nbsp;\n",
        "    <a target=\"_blank\" href=\"https://github.com/GokuMohandas/MadeWithML\"><img src=\"https://img.shields.io/github/stars/GokuMohandas/MadeWithML.svg?style=social&label=Star\"></a>&nbsp;\n",
        "    <a target=\"_blank\" href=\"https://www.linkedin.com/in/goku\"><img src=\"https://img.shields.io/badge/style--5eba00.svg?label=LinkedIn&logo=linkedin&style=social\"></a>&nbsp;\n",
        "    <a target=\"_blank\" href=\"https://twitter.com/GokuMohandas\"><img src=\"https://img.shields.io/twitter/follow/GokuMohandas.svg?label=Follow&style=social\"></a>\n",
        "    <br>\n",
        "    🔥&nbsp; Among the <a href=\"https://github.com/GokuMohandas/MadeWithML\" target=\"_blank\">top ML</a> repositories on GitHub\n",
        "</div>\n",
        "\n",
        "<br>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oh-HuNfDrPg0"
      },
      "source": [
        "This notebooks contains the code for the 🔢&nbsp; Data and 📈&nbsp; Modeling lessons. After this proof of concept (PoC), we'll be moving all of this code to Python scripts to serve our application to production. Follow the accompanying [lessons](https://madewithml.com/) along with the code here to develop a deeper understanding of all the concepts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTNsIiUrqoJW"
      },
      "source": [
        "<div align=\"left\">\n",
        "<a target=\"_blank\" href=\"https://madewithml.com/\"><img src=\"https://img.shields.io/badge/📖 Read-lessons-9cf\"></a>&nbsp;\n",
        "<a href=\"https://github.com/GokuMohandas/Made-With-ML/blob/main/notebooks/madewithml.ipynb\" role=\"button\"><img src=\"https://img.shields.io/static/v1?label=&amp;message=View%20On%20GitHub&amp;color=586069&amp;logo=github&amp;labelColor=2f363d\"></a>&nbsp;\n",
        "<a href=\"https://colab.research.google.com/github/GokuMohandas/Made-With-ML/blob/main/notebooks/madewithml.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "vXp_1oAivjul"
      },
      "source": [
        "# 🛠️&nbsp; Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FUq-GP4vjul"
      },
      "source": [
        "We'll be using [Ray](https://ray.io) to develop our application using distributed workloads."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "YCdK_lIDvjul",
        "outputId": "70ba22e6-bf44-4145-e4bf-a303046eebf0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hyperopt==0.2.7 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (0.2.7)\n",
            "Collecting ipywidgets>=8 (from -r requirements.txt (line 3))\n",
            "  Downloading ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting matplotlib==3.7.1 (from -r requirements.txt (line 4))\n",
            "  Downloading matplotlib-3.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting mlflow==2.3.1 (from -r requirements.txt (line 5))\n",
            "  Downloading mlflow-2.3.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting nltk==3.8.1 (from -r requirements.txt (line 6))\n",
            "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting numpy==1.24.3 (from -r requirements.txt (line 7))\n",
            "  Downloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting numpyencoder==0.3.0 (from -r requirements.txt (line 8))\n",
            "  Downloading numpyencoder-0.3.0-py3-none-any.whl.metadata (871 bytes)\n",
            "Collecting pandas==2.0.1 (from -r requirements.txt (line 9))\n",
            "  Downloading pandas-2.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting pretty-errors==1.2.25 (from -r requirements.txt (line 10))\n",
            "  Downloading pretty_errors-1.2.25-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting ray==2.6.0 (from ray[air]==2.6.0->-r requirements.txt (line 11))\n",
            "  Downloading ray-2.6.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting scikit-learn==1.2.2 (from -r requirements.txt (line 12))\n",
            "  Downloading scikit_learn-1.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting snorkel==0.9.9 (from -r requirements.txt (line 13))\n",
            "  Downloading snorkel-0.9.9-py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting SQLAlchemy==1.4.48 (from -r requirements.txt (line 14))\n",
            "  Downloading SQLAlchemy-1.4.48-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting torch==2.0.0 (from -r requirements.txt (line 15))\n",
            "  Downloading torch-2.0.0-cp311-cp311-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Collecting transformers==4.28.1 (from -r requirements.txt (line 16))\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl.metadata (109 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.0/110.0 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cleanlab==2.3.1 (from -r requirements.txt (line 19))\n",
            "  Downloading cleanlab-2.3.1-py3-none-any.whl.metadata (34 kB)\n",
            "Collecting jupyterlab==3.6.3 (from -r requirements.txt (line 20))\n",
            "  Downloading jupyterlab-3.6.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting lime==0.2.0.1 (from -r requirements.txt (line 21))\n",
            "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting seaborn==0.12.2 (from -r requirements.txt (line 22))\n",
            "  Downloading seaborn-0.12.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting wordcloud==1.9.2 (from -r requirements.txt (line 23))\n",
            "  Downloading wordcloud-1.9.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting mkdocs==1.4.2 (from -r requirements.txt (line 26))\n",
            "  Downloading mkdocs-1.4.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting mkdocstrings==0.21.2 (from -r requirements.txt (line 27))\n",
            "  Downloading mkdocstrings-0.21.2-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting black==23.3.0 (from -r requirements.txt (line 31))\n",
            "  Downloading black-23.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting flake8==6.0.0 (from -r requirements.txt (line 32))\n",
            "  Downloading flake8-6.0.0-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting Flake8-pyproject==1.2.3 (from -r requirements.txt (line 33))\n",
            "  Downloading flake8_pyproject-1.2.3-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting isort==5.12.0 (from -r requirements.txt (line 34))\n",
            "  Downloading isort-5.12.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting pyupgrade==3.3.2 (from -r requirements.txt (line 35))\n",
            "  Downloading pyupgrade-3.3.2-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Collecting great-expectations==0.16.5 (from -r requirements.txt (line 38))\n",
            "  Downloading great_expectations-0.16.5-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pytest==7.3.1 (from -r requirements.txt (line 39))\n",
            "  Downloading pytest-7.3.1-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting pytest-cov==4.0.0 (from -r requirements.txt (line 40))\n",
            "  Downloading pytest_cov-4.0.0-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting fastapi==0.95.2 (from -r requirements.txt (line 43))\n",
            "  Downloading fastapi-0.95.2-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting pre-commit==3.2.2 (from -r requirements.txt (line 44))\n",
            "  Downloading pre_commit-3.2.2-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting typer==0.9.0 (from -r requirements.txt (line 45))\n",
            "  Downloading typer-0.9.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting anyscale==0.5.131 (from -r requirements.txt (line 48))\n",
            "  Downloading anyscale-0.5.131.tar.gz (809 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m809.1/809.1 kB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from hyperopt==0.2.7->-r requirements.txt (line 2)) (1.15.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from hyperopt==0.2.7->-r requirements.txt (line 2)) (1.17.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.11/dist-packages (from hyperopt==0.2.7->-r requirements.txt (line 2)) (3.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from hyperopt==0.2.7->-r requirements.txt (line 2)) (1.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from hyperopt==0.2.7->-r requirements.txt (line 2)) (4.67.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from hyperopt==0.2.7->-r requirements.txt (line 2)) (3.1.1)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.11/dist-packages (from hyperopt==0.2.7->-r requirements.txt (line 2)) (0.10.9.7)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1->-r requirements.txt (line 4)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1->-r requirements.txt (line 4)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1->-r requirements.txt (line 4)) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1->-r requirements.txt (line 4)) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1->-r requirements.txt (line 4)) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1->-r requirements.txt (line 4)) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1->-r requirements.txt (line 4)) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1->-r requirements.txt (line 4)) (2.9.0.post0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.3.1->-r requirements.txt (line 5)) (8.2.1)\n",
            "Collecting cloudpickle (from hyperopt==0.2.7->-r requirements.txt (line 2))\n",
            "  Downloading cloudpickle-2.2.1-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting databricks-cli<1,>=0.8.7 (from mlflow==2.3.1->-r requirements.txt (line 5))\n",
            "  Downloading databricks_cli-0.18.0-py2.py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: entrypoints<1 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.3.1->-r requirements.txt (line 5)) (0.4)\n",
            "Requirement already satisfied: gitpython<4,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.3.1->-r requirements.txt (line 5)) (3.1.44)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.3.1->-r requirements.txt (line 5)) (6.0.2)\n",
            "Collecting protobuf<5,>=3.12.0 (from mlflow==2.3.1->-r requirements.txt (line 5))\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting pytz<2024 (from mlflow==2.3.1->-r requirements.txt (line 5))\n",
            "  Downloading pytz-2023.4-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.3.1->-r requirements.txt (line 5)) (2.32.3)\n",
            "Collecting packaging>=20.0 (from matplotlib==3.7.1->-r requirements.txt (line 4))\n",
            "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting importlib-metadata!=4.7.0,<7,>=3.7.0 (from mlflow==2.3.1->-r requirements.txt (line 5))\n",
            "  Downloading importlib_metadata-6.11.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.3.1->-r requirements.txt (line 5)) (0.5.3)\n",
            "Collecting alembic!=1.10.0,<2 (from mlflow==2.3.1->-r requirements.txt (line 5))\n",
            "  Downloading alembic-1.16.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting docker<7,>=4.0.0 (from mlflow==2.3.1->-r requirements.txt (line 5))\n",
            "  Downloading docker-6.1.3-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting Flask<3 (from mlflow==2.3.1->-r requirements.txt (line 5))\n",
            "  Downloading flask-2.3.3-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting querystring-parser<2 (from mlflow==2.3.1->-r requirements.txt (line 5))\n",
            "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl.metadata (559 bytes)\n",
            "Collecting pyarrow<12,>=4.0.0 (from mlflow==2.3.1->-r requirements.txt (line 5))\n",
            "  Downloading pyarrow-11.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.3.1->-r requirements.txt (line 5)) (3.8)\n",
            "Collecting gunicorn<21 (from mlflow==2.3.1->-r requirements.txt (line 5))\n",
            "  Downloading gunicorn-20.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.3.1->-r requirements.txt (line 5)) (3.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1->-r requirements.txt (line 6)) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1->-r requirements.txt (line 6)) (2024.11.6)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.0.1->-r requirements.txt (line 9)) (2025.2)\n",
            "Collecting colorama (from pretty-errors==1.2.25->-r requirements.txt (line 10))\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from ray==2.6.0->ray[air]==2.6.0->-r requirements.txt (line 11)) (3.18.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from ray==2.6.0->ray[air]==2.6.0->-r requirements.txt (line 11)) (4.24.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray==2.6.0->ray[air]==2.6.0->-r requirements.txt (line 11)) (1.1.1)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.11/dist-packages (from ray==2.6.0->ray[air]==2.6.0->-r requirements.txt (line 11)) (1.3.2)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.11/dist-packages (from ray==2.6.0->ray[air]==2.6.0->-r requirements.txt (line 11)) (1.7.0)\n",
            "Requirement already satisfied: grpcio>=1.42.0 in /usr/local/lib/python3.11/dist-packages (from ray==2.6.0->ray[air]==2.6.0->-r requirements.txt (line 11)) (1.73.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2->-r requirements.txt (line 12)) (3.6.0)\n",
            "Collecting munkres>=1.0.6 (from snorkel==0.9.9->-r requirements.txt (line 13))\n",
            "  Downloading munkres-1.1.4-py2.py3-none-any.whl.metadata (980 bytes)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.11/dist-packages (from snorkel==0.9.9->-r requirements.txt (line 13)) (2.18.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy==1.4.48->-r requirements.txt (line 14)) (3.2.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->-r requirements.txt (line 15)) (4.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->-r requirements.txt (line 15)) (1.13.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.0->-r requirements.txt (line 15))\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.0->-r requirements.txt (line 15))\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.0->-r requirements.txt (line 15))\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.0->-r requirements.txt (line 15))\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.0->-r requirements.txt (line 15))\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.0->-r requirements.txt (line 15))\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.0->-r requirements.txt (line 15))\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.0->-r requirements.txt (line 15))\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.0->-r requirements.txt (line 15))\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.0->-r requirements.txt (line 15))\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.0->-r requirements.txt (line 15))\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.0.0 (from torch==2.0.0->-r requirements.txt (line 15))\n",
            "  Downloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.1->-r requirements.txt (line 16)) (0.33.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.28.1->-r requirements.txt (line 16))\n",
            "  Downloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: termcolor>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from cleanlab==2.3.1->-r requirements.txt (line 19)) (3.1.0)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (from jupyterlab==3.6.3->-r requirements.txt (line 20)) (7.34.0)\n",
            "Requirement already satisfied: tornado>=6.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab==3.6.3->-r requirements.txt (line 20)) (6.4.2)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.11/dist-packages (from jupyterlab==3.6.3->-r requirements.txt (line 20)) (5.8.1)\n",
            "Collecting jupyterlab-server~=2.19 (from jupyterlab==3.6.3->-r requirements.txt (line 20))\n",
            "  Downloading jupyterlab_server-2.27.3-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab==3.6.3->-r requirements.txt (line 20)) (1.16.0)\n",
            "Collecting jupyter-ydoc~=0.2.3 (from jupyterlab==3.6.3->-r requirements.txt (line 20))\n",
            "  Downloading jupyter_ydoc-0.2.5-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting jupyter-server-ydoc~=0.8.0 (from jupyterlab==3.6.3->-r requirements.txt (line 20))\n",
            "  Downloading jupyter_server_ydoc-0.8.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: nbclassic in /usr/local/lib/python3.11/dist-packages (from jupyterlab==3.6.3->-r requirements.txt (line 20)) (1.3.1)\n",
            "Requirement already satisfied: notebook<7 in /usr/local/lib/python3.11/dist-packages (from jupyterlab==3.6.3->-r requirements.txt (line 20)) (6.5.7)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.11/dist-packages (from lime==0.2.0.1->-r requirements.txt (line 21)) (0.25.2)\n",
            "Collecting ghp-import>=1.0 (from mkdocs==1.4.2->-r requirements.txt (line 26))\n",
            "  Downloading ghp_import-2.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting markdown<4,>=3.3 (from mlflow==2.3.1->-r requirements.txt (line 5))\n",
            "  Downloading Markdown-3.3.7-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting mergedeep>=1.3.4 (from mkdocs==1.4.2->-r requirements.txt (line 26))\n",
            "  Downloading mergedeep-1.3.4-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting pyyaml-env-tag>=0.1 (from mkdocs==1.4.2->-r requirements.txt (line 26))\n",
            "  Downloading pyyaml_env_tag-1.1-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting watchdog>=2.0 (from mkdocs==1.4.2->-r requirements.txt (line 26))\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=1.1 in /usr/local/lib/python3.11/dist-packages (from mkdocstrings==0.21.2->-r requirements.txt (line 27)) (3.0.2)\n",
            "Collecting mkdocs-autorefs>=0.3.1 (from mkdocstrings==0.21.2->-r requirements.txt (line 27))\n",
            "  Downloading mkdocs_autorefs-1.4.2-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pymdown-extensions>=6.3 (from mkdocstrings==0.21.2->-r requirements.txt (line 27))\n",
            "  Downloading pymdown_extensions-10.16-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting mypy-extensions>=0.4.3 (from black==23.3.0->-r requirements.txt (line 31))\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting pathspec>=0.9.0 (from black==23.3.0->-r requirements.txt (line 31))\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.11/dist-packages (from black==23.3.0->-r requirements.txt (line 31)) (4.3.8)\n",
            "Collecting mccabe<0.8.0,>=0.7.0 (from flake8==6.0.0->-r requirements.txt (line 32))\n",
            "  Downloading mccabe-0.7.0-py2.py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting pycodestyle<2.11.0,>=2.10.0 (from flake8==6.0.0->-r requirements.txt (line 32))\n",
            "  Downloading pycodestyle-2.10.0-py2.py3-none-any.whl.metadata (31 kB)\n",
            "Collecting pyflakes<3.1.0,>=3.0.0 (from flake8==6.0.0->-r requirements.txt (line 32))\n",
            "  Downloading pyflakes-3.0.1-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting tokenize-rt>=3.2.0 (from pyupgrade==3.3.2->-r requirements.txt (line 35))\n",
            "  Downloading tokenize_rt-6.2.0-py2.py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting altair<4.2.1,>=4.0.0 (from great-expectations==0.16.5->-r requirements.txt (line 38))\n",
            "  Downloading altair-4.2.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: cryptography>=3.2 in /usr/local/lib/python3.11/dist-packages (from great-expectations==0.16.5->-r requirements.txt (line 38)) (43.0.3)\n",
            "Requirement already satisfied: jsonpatch>=1.22 in /usr/local/lib/python3.11/dist-packages (from great-expectations==0.16.5->-r requirements.txt (line 38)) (1.33)\n",
            "Collecting makefun<2,>=1.7.0 (from great-expectations==0.16.5->-r requirements.txt (line 38))\n",
            "  Downloading makefun-1.16.0-py2.py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.7.1 (from great-expectations==0.16.5->-r requirements.txt (line 38))\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: mistune>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from great-expectations==0.16.5->-r requirements.txt (line 38)) (3.1.3)\n",
            "Requirement already satisfied: nbformat>=5.0 in /usr/local/lib/python3.11/dist-packages (from great-expectations==0.16.5->-r requirements.txt (line 38)) (5.10.4)\n",
            "Collecting pydantic<2.0,>=1.9.2 (from great-expectations==0.16.5->-r requirements.txt (line 38))\n",
            "  Downloading pydantic-1.10.22-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.1/154.1 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ruamel.yaml<0.17.18,>=0.16 (from great-expectations==0.16.5->-r requirements.txt (line 38))\n",
            "  Downloading ruamel.yaml-0.17.17-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: tzlocal>=1.2 in /usr/local/lib/python3.11/dist-packages (from great-expectations==0.16.5->-r requirements.txt (line 38)) (5.3.1)\n",
            "Collecting urllib3<1.27,>=1.25.4 (from great-expectations==0.16.5->-r requirements.txt (line 38))\n",
            "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest==7.3.1->-r requirements.txt (line 39)) (2.1.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from pytest==7.3.1->-r requirements.txt (line 39)) (1.6.0)\n",
            "Collecting coverage>=5.2.1 (from coverage[toml]>=5.2.1->pytest-cov==4.0.0->-r requirements.txt (line 40))\n",
            "  Downloading coverage-7.9.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.9 kB)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi==0.95.2->-r requirements.txt (line 43))\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting cfgv>=2.0.0 (from pre-commit==3.2.2->-r requirements.txt (line 44))\n",
            "  Downloading cfgv-3.4.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting identify>=1.0.0 (from pre-commit==3.2.2->-r requirements.txt (line 44))\n",
            "  Downloading identify-2.6.12-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting nodeenv>=0.11.1 (from pre-commit==3.2.2->-r requirements.txt (line 44))\n",
            "  Downloading nodeenv-1.9.1-py2.py3-none-any.whl.metadata (21 kB)\n",
            "Collecting virtualenv>=20.10.0 (from pre-commit==3.2.2->-r requirements.txt (line 44))\n",
            "  Downloading virtualenv-20.31.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting boto3>=1.16.52 (from anyscale==0.5.131->-r requirements.txt (line 48))\n",
            "  Downloading boto3-1.38.42-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting botocore>=1.19.52 (from anyscale==0.5.131->-r requirements.txt (line 48))\n",
            "  Downloading botocore-1.38.42-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: aiohttp>=3.7.4.post0 in /usr/local/lib/python3.11/dist-packages (from anyscale==0.5.131->-r requirements.txt (line 48)) (3.11.15)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from anyscale==0.5.131->-r requirements.txt (line 48)) (2025.6.15)\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.11/dist-packages (from anyscale==0.5.131->-r requirements.txt (line 48)) (2.38.0)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.11/dist-packages (from anyscale==0.5.131->-r requirements.txt (line 48)) (4.1.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from anyscale==0.5.131->-r requirements.txt (line 48)) (13.9.4)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from anyscale==0.5.131->-r requirements.txt (line 48)) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from anyscale==0.5.131->-r requirements.txt (line 48)) (1.17.2)\n",
            "Requirement already satisfied: smart_open in /usr/local/lib/python3.11/dist-packages (from anyscale==0.5.131->-r requirements.txt (line 48)) (7.1.0)\n",
            "Collecting halo (from anyscale==0.5.131->-r requirements.txt (line 48))\n",
            "  Downloading halo-0.0.31.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.11/dist-packages (from anyscale==0.5.131->-r requirements.txt (line 48)) (4.12.3)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from anyscale==0.5.131->-r requirements.txt (line 48)) (25.1.0)\n",
            "Collecting opencensus (from ray[air]==2.6.0->-r requirements.txt (line 11))\n",
            "  Downloading opencensus-0.11.4-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from ray[air]==2.6.0->-r requirements.txt (line 11)) (2025.3.2)\n",
            "Requirement already satisfied: tensorboardX>=1.9 in /usr/local/lib/python3.11/dist-packages (from ray[air]==2.6.0->-r requirements.txt (line 11)) (2.6.4)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (from ray[air]==2.6.0->-r requirements.txt (line 11)) (0.34.3)\n",
            "Collecting aiorwlock (from ray[air]==2.6.0->-r requirements.txt (line 11))\n",
            "  Downloading aiorwlock-1.5.0-py3-none-any.whl.metadata (950 bytes)\n",
            "Collecting gpustat>=1.0.0 (from ray[air]==2.6.0->-r requirements.txt (line 11))\n",
            "  Downloading gpustat-1.1.1.tar.gz (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.1/98.1 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install all required packages for the MLOps course\n",
        "!pip install -r requirements.txt\n",
        "!pip install ray[tune] pycaret dvc\n",
        "\n",
        "\n",
        "!pip install 'ray[tune]'\n",
        "\n",
        "\n",
        "import ray\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "bATcKxykvjun"
      },
      "outputs": [],
      "source": [
        "# Initialize Ray\n",
        "if ray.is_initialized():\n",
        "    ray.shutdown()\n",
        "ray.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "a7Rju1qnvjun"
      },
      "outputs": [],
      "source": [
        "ray.cluster_resources()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VXosxKlvjuo"
      },
      "source": [
        "These cluster resources only reflect our head node ([m5.2xlarge](https://instances.vantage.sh/aws/ec2/m5.2xlarge)). But recall in our [setup lesson](https://madewithml.com/courses/mlops/setup/) that our [compute configuration](https://madewithml.com/courses/mlops/setup/#compute) that we also added [g4dn.xlarge](https://instances.vantage.sh/aws/ec2/g4dn.xlarge) worker nodes (each has 1 GPU and 4 CPU) to our cluster. But because we set `min_workers=0`, our worker nodes will autoscale ( up to `max_workers`) as they're needed for specific workloads (ex. training)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "zdmZZS4lvjuo"
      },
      "outputs": [],
      "source": [
        "# Workers (1 g4dn.xlarge)\n",
        "num_workers = 1\n",
        "resources_per_worker={\"CPU\": 3, \"GPU\": 1}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moQEC0swvjuo"
      },
      "source": [
        "If you are running this on a local laptop (no GPU), use the CPU count from `ray.cluster_resources()` to set your resources. For example if your machine has 10 CPUs:\n",
        "\n",
        "```python\n",
        "num_workers = 6  # prefer to do a few less than total available CPU (1 for head node + 1 for background tasks)\n",
        "resources_per_worker={\"CPU\": 1, \"GPU\": 0}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vLc0b0tvjuo"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "4zs__CzLvjuo"
      },
      "source": [
        "## 🔢&nbsp; Data ingestion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "pVyr9V35vjup"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "w_33Iuqkvjup"
      },
      "outputs": [],
      "source": [
        "# Data ingestion\n",
        "DATASET_LOC = \"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/dataset.csv\"\n",
        "df = pd.read_csv(DATASET_LOC)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "KvJK-tXTvjup"
      },
      "source": [
        "## ✂️&nbsp; Data splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "g0SwnqlEvjup"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "kD3etLVgvjup"
      },
      "outputs": [],
      "source": [
        "# Value counts\n",
        "df.tag.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "DrXPh-EHvjup"
      },
      "outputs": [],
      "source": [
        "# Split dataset\n",
        "test_size = 0.2\n",
        "train_df, val_df = train_test_split(df, stratify=df.tag, test_size=test_size, random_state=1234)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "VRiFV_Ocvjup"
      },
      "outputs": [],
      "source": [
        "# Train value counts\n",
        "train_df.tag.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "uFzTr8WLvjuq"
      },
      "outputs": [],
      "source": [
        "# Validation (adjusted) value counts\n",
        "val_df.tag.value_counts() * int((1-test_size) / test_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuCrsbxbNkSV"
      },
      "source": [
        "## 🔍&nbsp; Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOJ3nlEgnSTJ"
      },
      "source": [
        "Exploratory data analysis to understand the signals and nuances of our dataset. It's a cyclical process that can be done at various points of our development process (before/after labeling, preprocessing, etc.) depending on how well the problem is defined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHdQmqTBNkSV",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set_theme()\n",
        "import warnings; warnings.filterwarnings(\"ignore\")\n",
        "from wordcloud import WordCloud, STOPWORDS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "Q0Y7cl9Svjuq"
      },
      "outputs": [],
      "source": [
        "# Most common tags\n",
        "all_tags = Counter(df.tag)\n",
        "all_tags.most_common()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gl-E8d2HaCsx",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Plot tag frequencies\n",
        "tags, tag_counts = zip(*all_tags.most_common())\n",
        "plt.figure(figsize=(10, 3))\n",
        "ax = sns.barplot(x=list(tags), y=list(tag_counts))\n",
        "ax.set_xticklabels(tags, rotation=0, fontsize=12)\n",
        "plt.title(\"Tag distribution\", fontsize=16)\n",
        "plt.ylabel(\"# of projects\", fontsize=14)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfjVstecaFC5"
      },
      "source": [
        "> We'll address the [data imbalance](https://madewithml.com/courses/mlops/baselines#data-imbalance) after splitting into our train split and prior to training our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NgMGuIQrNkSV",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Most frequent tokens for each tag\n",
        "tag=\"natural-language-processing\"\n",
        "plt.figure(figsize=(10, 3))\n",
        "subset = df[df.tag==tag]\n",
        "text = subset.title.values\n",
        "cloud = WordCloud(\n",
        "    stopwords=STOPWORDS, background_color=\"black\", collocations=False,\n",
        "    width=500, height=300).generate(\" \".join(text))\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(cloud)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8ua3MFhrOaX"
      },
      "source": [
        "Looks like the `title` text feature has some good signal for the respective classes and matches our intuition. We can repeat this for the `description` text feature as well. This information will become useful when we decide how to use our features for modeling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFifXKl_eKsN"
      },
      "source": [
        "## ✨&nbsp; Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxAZ1AmteRaD"
      },
      "source": [
        "Preprocessing the data via feature engineering, filtering and cleaning. Certain preprocessing steps are global (don't depend on our dataset, ex. lower casing text, removing stop words, etc.) and others are local (constructs are learned only from the training split, ex. vocabulary, standardization, etc.). For the local, dataset-dependent preprocessing steps, we want to ensure that we [split](https://madewithml.com/courses/mlops/splitting) the data first before preprocessing to avoid data leaks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "Vc2y-ww1vjur"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VgTwEQboTGc"
      },
      "source": [
        "### Feature engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_001GPyMZsC"
      },
      "source": [
        "We can combine existing input features to create new meaningful signal (helping the model learn)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3x1ldAFQNkSU",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Feature engineering\n",
        "df[\"text\"] = df.title + \" \" + df.description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZ1Lke5Nvjus"
      },
      "source": [
        "### Clean text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDXLH6QeLd0F",
        "tags": []
      },
      "outputs": [],
      "source": [
        "nltk.download(\"stopwords\")\n",
        "STOPWORDS = stopwords.words(\"english\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfdWkkV8LlNR",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def clean_text(text, stopwords=STOPWORDS):\n",
        "    \"\"\"Clean raw text string.\"\"\"\n",
        "    # Lower\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove stopwords\n",
        "    pattern = re.compile(r'\\b(' + r\"|\".join(stopwords) + r\")\\b\\s*\")\n",
        "    text = pattern.sub('', text)\n",
        "\n",
        "    # Spacing and filters\n",
        "    text = re.sub(r\"([!\\\"'#$%&()*\\+,-./:;<=>?@\\\\\\[\\]^_`{|}~])\", r\" \\1 \", text)  # add spacing\n",
        "    text = re.sub(\"[^A-Za-z0-9]+\", \" \", text)  # remove non alphanumeric chars\n",
        "    text = re.sub(\" +\", \" \", text)  # remove multiple spaces\n",
        "    text = text.strip()  # strip white space at the ends\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)  #  remove links\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LRaq0_5LpE4",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Apply to dataframe\n",
        "original_df = df.copy()\n",
        "df.text = df.text.apply(clean_text)\n",
        "print (f\"{original_df.text.values[0]}\\n{df.text.values[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLe2VQ0Dvjuz"
      },
      "source": [
        "### Clean DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "lb6EsBJNvjuz"
      },
      "outputs": [],
      "source": [
        "# DataFrame cleanup\n",
        "df = df.drop(columns=[\"id\", \"created_on\", \"title\", \"description\"], errors=\"ignore\")  # drop cols\n",
        "df = df.dropna(subset=[\"tag\"])  # drop nulls\n",
        "df = df[[\"text\", \"tag\"]]  # rearrange cols\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1pJJBVKvjuz"
      },
      "source": [
        "### Label encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNiFIVxavju0"
      },
      "source": [
        "We need to encode our data into numerical values so that our models can process them. We'll start by encoding our text labels into unique indices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "NDVzPa2Evju0"
      },
      "outputs": [],
      "source": [
        "# Label to index\n",
        "tags = train_df.tag.unique().tolist()\n",
        "num_classes = len(tags)\n",
        "class_to_index = {tag: i for i, tag in enumerate(tags)}\n",
        "class_to_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "6lsLKTVxvju0"
      },
      "outputs": [],
      "source": [
        "# Encode labels\n",
        "df[\"tag\"] = df[\"tag\"].map(class_to_index)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "2sLjv36jvju0"
      },
      "outputs": [],
      "source": [
        "def decode(indices, index_to_class):\n",
        "    return [index_to_class[index] for index in indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "zwhiWQufvju0"
      },
      "outputs": [],
      "source": [
        "index_to_class = {v:k for k, v in class_to_index.items()}\n",
        "decode(df.head()[\"tag\"].values, index_to_class=index_to_class)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZjR14vqvju1"
      },
      "source": [
        "### Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNHY9HCQvju1"
      },
      "source": [
        "Next we'll encode our text as well. Instead of using a random dictionary, we'll use a [tokenizer](https://huggingface.co/allenai/scibert_scivocab_uncased/blob/main/vocab.txt) that was used for a pretrained LLM ([scibert](https://huggingface.co/allenai/scibert_scivocab_uncased)) to tokenize our text. We'll be fine-tuning this exact model later when we train our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "kBYvGq_Tvju1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from transformers import BertTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "F_aGvrxevju1"
      },
      "outputs": [],
      "source": [
        "# Bert tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\", return_dict=False)\n",
        "text = \"Transfer learning with transformers for text classification.\"\n",
        "encoded_inputs = tokenizer([text], return_tensors=\"np\", padding=\"longest\")  # pad to longest item in batch\n",
        "print (\"input_ids:\", encoded_inputs[\"input_ids\"])\n",
        "print (\"attention_mask:\", encoded_inputs[\"attention_mask\"])\n",
        "print (tokenizer.decode(encoded_inputs[\"input_ids\"][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "qThHr8yQvju1"
      },
      "outputs": [],
      "source": [
        "def tokenize(batch):\n",
        "    tokenizer = BertTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\", return_dict=False)\n",
        "    encoded_inputs = tokenizer(batch[\"text\"].tolist(), return_tensors=\"np\", padding=\"longest\")\n",
        "    return dict(ids=encoded_inputs[\"input_ids\"], masks=encoded_inputs[\"attention_mask\"], targets=np.array(batch[\"tag\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "aa_YbUTIvju1"
      },
      "outputs": [],
      "source": [
        "# Tokenization\n",
        "tokenize(df.head(1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yINHR7hUvju2"
      },
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPuwEPt7vju2"
      },
      "source": [
        "We'll combine all of our preprocessing steps into function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "vM2Xbz4Ovju2"
      },
      "outputs": [],
      "source": [
        "def preprocess(df, class_to_index):\n",
        "    \"\"\"Preprocess the data.\"\"\"\n",
        "    df[\"text\"] = df.title + \" \" + df.description  # feature engineering\n",
        "    df[\"text\"] = df.text.apply(clean_text)  # clean text\n",
        "    df = df.drop(columns=[\"id\", \"created_on\", \"title\", \"description\"], errors=\"ignore\")  # clean dataframe\n",
        "    df = df[[\"text\", \"tag\"]]  # rearrange columns\n",
        "    df[\"tag\"] = df[\"tag\"].map(class_to_index)  # label encoding\n",
        "    outputs = tokenize(df)\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "rcl2jnwUvju2"
      },
      "outputs": [],
      "source": [
        "# Apply\n",
        "preprocess(df=train_df, class_to_index=class_to_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_o4oXd89vju2"
      },
      "source": [
        "### Distributed preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eECv1C6_vju2"
      },
      "source": [
        "The main issue with our approach above is that we're limited by our single machine in terms how much data our dataframe can hold and that we can preprocess. With the increasing trend in ML for larger unstructured datasets and larger models (LLMs), we can quickly outgrow our single machine constraints and will need to go distributed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "RqvDM6eVvju3"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/GokuMohandas/mlops-course.git\n",
        "%cd mlops-course\n",
        "\n",
        "\n",
        "\n",
        "import sys\n",
        "sys.path.append(\".\")\n",
        "\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "from madewithml.data import stratify_split\n",
        "ray.data.DatasetContext.get_current().execution_options.preserve_order = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "_V530Op5vju3"
      },
      "outputs": [],
      "source": [
        "# Data ingestion\n",
        "ds = ray.data.read_csv(DATASET_LOC)\n",
        "ds = ds.random_shuffle(seed=1234)\n",
        "ds.take(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "oaj6-AfJvju3"
      },
      "outputs": [],
      "source": [
        "# Split dataset\n",
        "test_size = 0.2\n",
        "train_ds, val_ds = stratify_split(ds, stratify=\"tag\", test_size=test_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "PSi6tvayvju3"
      },
      "outputs": [],
      "source": [
        "# Mapping\n",
        "tags = train_ds.unique(column=\"tag\")\n",
        "class_to_index = {tag: i for i, tag in enumerate(tags)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWzw1nsRvju3"
      },
      "outputs": [],
      "source": [
        "# Distributed preprocessing\n",
        "sample_ds = train_ds.map_batches(preprocess, fn_kwargs={\"class_to_index\": class_to_index}, batch_format=\"pandas\")\n",
        "sample_ds.show(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGvI2YuuNkSX",
        "tags": []
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtBjFoVTvju4"
      },
      "source": [
        "When developing models, it's always a best practice to start with the simplest models and slowly motivate more complex models. For example our baseline model progression would be:\n",
        "\n",
        "1. random model (predict labels randomly)\n",
        "2. rule-based model (pattern match labels in input text)\n",
        "3. logistic regression (td-idf vectors from text)\n",
        "4. CNN (apply character filters over text)\n",
        "5. Fine-tune LLM (this notebook)\n",
        "\n",
        "We cover all of these methods in our [other lessons](https://madewithml.com/#foundations) but since our focus here in on MLOps, we will skip directly to fine-tuning an LLM for our task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9CfUuNh2YLE"
      },
      "source": [
        "We'll first set up some functions that will help us achieve complete reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86sFERmsuPQl",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "from ray.data.preprocessor import Preprocessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXd8flJuNkSY",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def set_seeds(seed=42):\n",
        "    \"\"\"Set seeds for reproducibility.\"\"\"\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    eval(\"setattr(torch.backends.cudnn, 'deterministic', True)\")\n",
        "    eval(\"setattr(torch.backends.cudnn, 'benchmark', False)\")\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "CxNQVat2vju4"
      },
      "outputs": [],
      "source": [
        "def load_data(num_samples=None):\n",
        "    ds = ray.data.read_csv(DATASET_LOC)\n",
        "    ds = ds.random_shuffle(seed=1234)\n",
        "    ds = ray.data.from_items(ds.take(num_samples)) if num_samples else ds\n",
        "    return ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "xX6ijt3Hvju4"
      },
      "outputs": [],
      "source": [
        "class CustomPreprocessor(Preprocessor):\n",
        "    \"\"\"Custom preprocessor class.\"\"\"\n",
        "    def _fit(self, ds):\n",
        "        tags = ds.unique(column=\"tag\")\n",
        "        self.class_to_index = {tag: i for i, tag in enumerate(tags)}\n",
        "        self.index_to_class = {v:k for k, v in self.class_to_index.items()}\n",
        "    def _transform_pandas(self, batch):  # could also do _transform_numpy\n",
        "        return preprocess(batch, class_to_index=self.class_to_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihgFPEVcvju5"
      },
      "source": [
        "## 🤖&nbsp; Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "6mguzFMJvju5"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from transformers import BertModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "34XakrpIvju5"
      },
      "outputs": [],
      "source": [
        "# Pretrained LLM\n",
        "llm = BertModel.from_pretrained(\"allenai/scibert_scivocab_uncased\", return_dict=False)\n",
        "embedding_dim = llm.config.hidden_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "c5p4juyrvju5"
      },
      "outputs": [],
      "source": [
        "# Sample\n",
        "text = \"Transfer learning with transformers for text classification.\"\n",
        "batch = tokenizer([text], return_tensors=\"np\", padding=\"longest\")\n",
        "batch = {k:torch.tensor(v) for k,v in batch.items()}  # convert to torch tensors\n",
        "seq, pool = llm(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
        "np.shape(seq), np.shape(pool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "q4Nz6qwBvju5"
      },
      "outputs": [],
      "source": [
        "class FinetunedLLM(nn.Module):\n",
        "    def __init__(self, llm, dropout_p, embedding_dim, num_classes):\n",
        "        super(FinetunedLLM, self).__init__()\n",
        "        self.llm = llm\n",
        "        self.dropout = torch.nn.Dropout(dropout_p)\n",
        "        self.fc1 = torch.nn.Linear(embedding_dim, num_classes)\n",
        "\n",
        "    def forward(self, batch):\n",
        "        ids, masks = batch[\"ids\"], batch[\"masks\"]\n",
        "        seq, pool = self.llm(input_ids=ids, attention_mask=masks)\n",
        "        z = self.dropout(pool)\n",
        "        z = self.fc1(z)\n",
        "        return z\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def predict(self, batch):\n",
        "        self.eval()\n",
        "        z = self(inputs)\n",
        "        y_pred = torch.argmax(z, dim=1).cpu().numpy()\n",
        "        return y_pred\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def predict_proba(self, batch):\n",
        "        self.eval()\n",
        "        z = self(batch)\n",
        "        y_probs = F.softmax(z).cpu().numpy()\n",
        "        return y_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "jR8DyS28vju5"
      },
      "outputs": [],
      "source": [
        "# Initialize model\n",
        "model = FinetunedLLM(llm=llm, dropout_p=0.5, embedding_dim=embedding_dim, num_classes=num_classes)\n",
        "print (model.named_parameters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SB6YFSdVvju6"
      },
      "source": [
        "## 📦&nbsp; Batching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zehOVzAPvju6"
      },
      "source": [
        "We can iterate through our dataset in batches however we may have batches of different sizes. Recall that our tokenizer padded the inputs to the longest item in the batch (`padding=\"longest\"`). However, our batches for training will be smaller than our large data processing batches and so our batches here may have inputs with different lengths. To address this, we're going to define a custom `collate_fn` to repad the items in our training batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "HSbb0DY_vju6"
      },
      "outputs": [],
      "source": [
        "from ray.train.torch import get_device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "eIvXQviNvju6"
      },
      "outputs": [],
      "source": [
        "def pad_array(arr, dtype=np.int32):\n",
        "    max_len = max(len(row) for row in arr)\n",
        "    padded_arr = np.zeros((arr.shape[0], max_len), dtype=dtype)\n",
        "    for i, row in enumerate(arr):\n",
        "        padded_arr[i][:len(row)] = row\n",
        "    return padded_arr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "OBOZKzUjvju6"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    batch[\"ids\"] = pad_array(batch[\"ids\"])\n",
        "    batch[\"masks\"] = pad_array(batch[\"masks\"])\n",
        "    dtypes = {\"ids\": torch.int32, \"masks\": torch.int32, \"targets\": torch.int64}\n",
        "    tensor_batch = {}\n",
        "    for key, array in batch.items():\n",
        "        tensor_batch[key] = torch.as_tensor(array, dtype=dtypes[key], device=get_device())\n",
        "    return tensor_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3f24N52vju7"
      },
      "source": [
        "> `pad=(0, max_len)` in [F.pad](https://pytorch.org/docs/stable/generated/torch.nn.functional.pad.html#torch-nn-functional-pad) refers to (left_padding, right_padding) on the input. There will be no left-padding (hence the `0`) and only right-padding. And the `constant` mode refers to each element being padded to a constant size (size of longest element in the input)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "VQ0UT8Mvvju7"
      },
      "outputs": [],
      "source": [
        "# Sample batch\n",
        "sample_batch = sample_ds.take_batch(batch_size=128)\n",
        "collate_fn(batch=sample_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHqlqLDgvju7"
      },
      "source": [
        "## 🧮&nbsp; Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "B5xZd2z6vju7"
      },
      "outputs": [],
      "source": [
        "from ray.air import Checkpoint, session\n",
        "from ray.air.config import CheckpointConfig, DatasetConfig, RunConfig, ScalingConfig\n",
        "import ray.train as train\n",
        "from ray.train.torch import TorchCheckpoint, TorchTrainer\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "9zd78Tyvvju7"
      },
      "outputs": [],
      "source": [
        "def train_step(ds, batch_size, model, num_classes, loss_fn, optimizer):\n",
        "    \"\"\"Train step.\"\"\"\n",
        "    model.train()\n",
        "    loss = 0.0\n",
        "    ds_generator = ds.iter_torch_batches(batch_size=batch_size, collate_fn=collate_fn)\n",
        "    for i, batch in enumerate(ds_generator):\n",
        "        optimizer.zero_grad()  # reset gradients\n",
        "        z = model(batch)  # forward pass\n",
        "        targets = F.one_hot(batch[\"targets\"], num_classes=num_classes).float()  # one-hot (for loss_fn)\n",
        "        J = loss_fn(z, targets)  # define loss\n",
        "        J.backward()  # backward pass\n",
        "        optimizer.step()  # update weights\n",
        "        loss += (J.detach().item() - loss) / (i + 1)  # cumulative loss\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "faPK9_Gcvju7"
      },
      "outputs": [],
      "source": [
        "def eval_step(ds, batch_size, model, num_classes, loss_fn):\n",
        "    \"\"\"Eval step.\"\"\"\n",
        "    model.eval()\n",
        "    loss = 0.0\n",
        "    y_trues, y_preds = [], []\n",
        "    ds_generator = ds.iter_torch_batches(batch_size=batch_size, collate_fn=collate_fn)\n",
        "    with torch.inference_mode():\n",
        "        for i, batch in enumerate(ds_generator):\n",
        "            z = model(batch)\n",
        "            targets = F.one_hot(batch[\"targets\"], num_classes=num_classes).float()  # one-hot (for loss_fn)\n",
        "            J = loss_fn(z, targets).item()\n",
        "            loss += (J - loss) / (i + 1)\n",
        "            y_trues.extend(batch[\"targets\"].cpu().numpy())\n",
        "            y_preds.extend(torch.argmax(z, dim=1).cpu().numpy())\n",
        "    return loss, np.vstack(y_trues), np.vstack(y_preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "JCs1icCUvju8"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "def train_loop_per_worker(config):\n",
        "    # Hyperparameters\n",
        "    dropout_p = config[\"dropout_p\"]\n",
        "    lr = config[\"lr\"]\n",
        "    lr_factor = config[\"lr_factor\"]\n",
        "    lr_patience = config[\"lr_patience\"]\n",
        "    num_epochs = config[\"num_epochs\"]\n",
        "    batch_size = config[\"batch_size\"]\n",
        "    num_classes = config[\"num_classes\"]\n",
        "\n",
        "    # Get datasets\n",
        "    set_seeds()\n",
        "    train_ds = session.get_dataset_shard(\"train\")\n",
        "    val_ds = session.get_dataset_shard(\"val\")\n",
        "\n",
        "    # Model\n",
        "    llm = BertModel.from_pretrained(\"allenai/scibert_scivocab_uncased\", return_dict=False)\n",
        "    model = FinetunedLLM(llm=llm, dropout_p=dropout_p, embedding_dim=llm.config.hidden_size, num_classes=num_classes)\n",
        "    model = train.torch.prepare_model(model)\n",
        "\n",
        "    # Training components\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=lr_factor, patience=lr_patience)\n",
        "\n",
        "    # Training\n",
        "    batch_size_per_worker = batch_size // session.get_world_size()\n",
        "    for epoch in range(num_epochs):\n",
        "        # Step\n",
        "        train_loss = train_step(train_ds, batch_size_per_worker, model, num_classes, loss_fn, optimizer)\n",
        "        val_loss, _, _ = eval_step(val_ds, batch_size_per_worker, model, num_classes, loss_fn)\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Checkpoint\n",
        "        metrics = dict(epoch=epoch, lr=optimizer.param_groups[0][\"lr\"], train_loss=train_loss, val_loss=val_loss)\n",
        "        checkpoint = TorchCheckpoint.from_model(model=model)\n",
        "        session.report(metrics, checkpoint=checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7cY_3ZQvju8"
      },
      "source": [
        "Our dataset doesn't suffer from horrible class imbalance, but if it did, we could easily account for it through our loss function. There are also other strategies such as [over-sampling](https://imbalanced-learn.org/stable/over_sampling.html) less frequent classes and [under-sampling](https://imbalanced-learn.org/stable/under_sampling.html) popular classes.\n",
        "\n",
        "```python\n",
        "# Class weights\n",
        "batch_counts = []\n",
        "for batch in train_ds.iter_torch_batches(batch_size=256, collate_fn=collate_fn):\n",
        "    batch_counts.append(np.bincount(batch[\"targets\"].cpu().numpy()))\n",
        "counts = [sum(count) for count in zip(*batch_counts)]\n",
        "class_weights = np.array([1.0/count for i, count in enumerate(counts)])\n",
        "class_weights_tensor = torch.Tensor(class_weights).to(get_device())\n",
        "\n",
        "# Training components\n",
        "loss_fn = nn.BCEWithLogitsLoss(weight=class_weights_tensor)\n",
        "...\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnR9jJRevju8"
      },
      "source": [
        "## 🗂️&nbsp; Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "J9__G-TBvju8"
      },
      "outputs": [],
      "source": [
        "# Train loop config\n",
        "train_loop_config = {\n",
        "    \"dropout_p\": 0.5,\n",
        "    \"lr\": 1e-4,\n",
        "    \"lr_factor\": 0.8,\n",
        "    \"lr_patience\": 3,\n",
        "    \"num_epochs\": 10,\n",
        "    \"batch_size\": 256,\n",
        "    \"num_classes\": num_classes,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "v-vB5AvCvju8"
      },
      "outputs": [],
      "source": [
        "# Scaling config\n",
        "scaling_config = ScalingConfig(\n",
        "    num_workers=num_workers,\n",
        "    use_gpu=bool(resources_per_worker[\"GPU\"]),\n",
        "    resources_per_worker=resources_per_worker,\n",
        "    _max_cpu_fraction_per_node=0.8,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "SeVmOk_Yvju9"
      },
      "outputs": [],
      "source": [
        "# Run config\n",
        "checkpoint_config = CheckpointConfig(num_to_keep=1, checkpoint_score_attribute=\"val_loss\", checkpoint_score_order=\"min\")\n",
        "run_config = RunConfig(name=\"llm\", checkpoint_config=checkpoint_config, local_dir=\"~/ray_results\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vewC2FjQvju9"
      },
      "source": [
        "## 🚂&nbsp; Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "12-LIVvxvju9"
      },
      "outputs": [],
      "source": [
        "# Dataset\n",
        "ds = load_data()\n",
        "train_ds, val_ds = stratify_split(ds, stratify=\"tag\", test_size=test_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "SbMgFoAuvju9"
      },
      "outputs": [],
      "source": [
        "# Preprocess\n",
        "preprocessor = CustomPreprocessor()\n",
        "train_ds =  preprocessor.fit_transform(train_ds)\n",
        "val_ds = preprocessor.transform(val_ds)\n",
        "train_ds = train_ds.materialize()\n",
        "val_ds = val_ds.materialize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "0chu-Jb9vju-"
      },
      "outputs": [],
      "source": [
        "# Dataset config\n",
        "dataset_config = {\n",
        "    \"train\": DatasetConfig(fit=False, transform=False, randomize_block_order=False),\n",
        "    \"val\": DatasetConfig(fit=False, transform=False, randomize_block_order=False),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "9EL5gjJKvju-"
      },
      "outputs": [],
      "source": [
        "# Trainer\n",
        "trainer = TorchTrainer(\n",
        "    train_loop_per_worker=train_loop_per_worker,\n",
        "    train_loop_config=train_loop_config,\n",
        "    scaling_config=scaling_config,\n",
        "    run_config=run_config,\n",
        "    datasets={\"train\": train_ds, \"val\": val_ds},\n",
        "    dataset_config=dataset_config,\n",
        "    preprocessor=preprocessor,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "3q-sMeOrvju-"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Train\n",
        "results = trainer.fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "2yHtZqnmvju-"
      },
      "outputs": [],
      "source": [
        "# Metrics per epoch\n",
        "results.metrics_dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "BnSlekRAvju-"
      },
      "outputs": [],
      "source": [
        "# Best checkpoints\n",
        "results.best_checkpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uy9yw8CDvju_"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "KFvQ-SrBvju_"
      },
      "outputs": [],
      "source": [
        "from ray.train.torch.torch_predictor import TorchPredictor\n",
        "from sklearn.metrics import precision_recall_fscore_support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "QaL-6XwFvju_"
      },
      "outputs": [],
      "source": [
        "# Predictor\n",
        "best_checkpoint = results.best_checkpoints[0][0]\n",
        "predictor = TorchPredictor.from_checkpoint(best_checkpoint)\n",
        "preprocessor = predictor.get_preprocessor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "fEb-inExvjvA"
      },
      "outputs": [],
      "source": [
        "# Test (holdout) dataset\n",
        "HOLDOUT_LOC = \"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/holdout.csv\"\n",
        "test_ds = ray.data.read_csv(HOLDOUT_LOC)\n",
        "preprocessed_ds = preprocessor.transform(test_ds)\n",
        "preprocessed_ds.take(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "5IlCMBAvvjvA"
      },
      "outputs": [],
      "source": [
        "# y_true\n",
        "values = preprocessed_ds.select_columns(cols=[\"targets\"]).take_all()\n",
        "y_true = np.stack([item[\"targets\"] for item in values])\n",
        "print (y_true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "T3RWPuD6vjvA"
      },
      "outputs": [],
      "source": [
        "# y_pred\n",
        "z = predictor.predict(data=test_ds.to_pandas())[\"predictions\"]\n",
        "y_pred = np.stack(z).argmax(1)\n",
        "print (y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "AYQV4r9HvjvB"
      },
      "outputs": [],
      "source": [
        "# Evaluate\n",
        "metrics = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
        "{\"precision\": metrics[0], \"recall\": metrics[1], \"f1\": metrics[2]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "MPwznyBrvjvB"
      },
      "outputs": [],
      "source": [
        "def evaluate(ds, predictor):\n",
        "    # y_true\n",
        "    preprocessor = predictor.get_preprocessor()\n",
        "    preprocessed_ds = preprocessor.transform(ds)\n",
        "    values = preprocessed_ds.select_columns(cols=[\"targets\"]).take_all()\n",
        "    y_true = np.stack([item[\"targets\"] for item in values])\n",
        "\n",
        "    # y_pred\n",
        "    z = predictor.predict(data=test_ds.to_pandas())[\"predictions\"]\n",
        "    y_pred = np.stack(z).argmax(1)\n",
        "\n",
        "    # Evaluate\n",
        "    metrics = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
        "    performance = {\"precision\": metrics[0], \"recall\": metrics[1], \"f1\": metrics[2]}\n",
        "    return performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "zTMSWNjDvjvB"
      },
      "outputs": [],
      "source": [
        "# Evaluate on test split\n",
        "performance = evaluate(ds=test_ds, predictor=predictor)\n",
        "print (json.dumps(performance, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ez5o1e3vjvB"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "wuJ_t5EvvjvB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "XX4475_kvjvC"
      },
      "outputs": [],
      "source": [
        "def format_prob(prob, index_to_class):\n",
        "    d = {}\n",
        "    for i, item in enumerate(prob):\n",
        "        d[index_to_class[i]] = item\n",
        "    return d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "-ydLxDVJvjvC"
      },
      "outputs": [],
      "source": [
        "def predict_with_proba(df, predictor):\n",
        "    preprocessor = predictor.get_preprocessor()\n",
        "    z = predictor.predict(data=df)[\"predictions\"]\n",
        "    y_prob = torch.tensor(np.stack(z)).softmax(dim=1).numpy()\n",
        "    results = []\n",
        "    for i, prob in enumerate(y_prob):\n",
        "        tag = decode([z[i].argmax()], preprocessor.index_to_class)[0]\n",
        "        results.append({\"prediction\": tag, \"probabilities\": format_prob(prob, preprocessor.index_to_class)})\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "DuqT-w_PvjvC"
      },
      "outputs": [],
      "source": [
        "# Predict on sample\n",
        "title = \"Transfer learning with transformers\"\n",
        "description = \"Using transformers for transfer learning on text classification tasks.\"\n",
        "sample_df = pd.DataFrame([{\"title\": title, \"description\": description, \"tag\": \"other\"}])\n",
        "predict_with_proba(df=sample_df, predictor=predictor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoe00ALgvjvC"
      },
      "source": [
        "# 🧪 Experiment tracking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9fwoIWWvjvD"
      },
      "source": [
        "So far, we've been training our models but we don't have a way to more deeply track and compare them. We'll achieve this but defining a proper process for experiment tracking which we'll use for all future experiments (including hyperparameter optimization). Experiment tracking is the processing of managing all the different experiments and their components, such as parameters, metrics, models and other artifacts and it enables us to:\n",
        "\n",
        "- **Organize** all the necessary components of a specific experiment. It's important to have everything in one place and know where it is so you can use them later.\n",
        "- **Reproduce** past results (easily) using saved experiments.\n",
        "- **Log** iterative improvements across time, data, ideas, teams, etc.\n",
        "\n",
        "There are many options for experiment tracking but we're going to use [MLflow](https://mlflow.org/) (100% free and [open-source](https://github.com/mlflow/mlflow)) because it has all the functionality we'll need (and [growing integration support](https://docs.ray.io/en/latest/tune/examples/tune-mlflow.html)). There are also several popular options such as a [Comet ML](https://www.comet.ml/site/) (Used by Google AI, HuggingFace, etc.) and [Weights and Biases](https://www.wandb.com/) (Used by Open AI, Toyota Research, etc.). These are fantastic options if you want a fully managed experiment tracking solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "DpjbhwlsvjvD"
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "from pathlib import Path\n",
        "from ray.air.integrations.mlflow import MLflowLoggerCallback\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "jnsnN9UVvjvD"
      },
      "outputs": [],
      "source": [
        "# Config MLflow\n",
        "MODEL_REGISTRY = Path(\"/tmp/mlflow\")\n",
        "Path(MODEL_REGISTRY).mkdir(parents=True, exist_ok=True)\n",
        "MLFLOW_TRACKING_URI = \"file://\" + str(MODEL_REGISTRY.absolute())\n",
        "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
        "print (mlflow.get_tracking_uri())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "Qq7xGa2nvjvE"
      },
      "outputs": [],
      "source": [
        "# MLflow callback\n",
        "experiment_name = f\"llm-{int(time.time())}\"\n",
        "mlflow_callback = MLflowLoggerCallback(\n",
        "    tracking_uri=MLFLOW_TRACKING_URI,\n",
        "    experiment_name=experiment_name,\n",
        "    save_artifact=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "FTnJQCtEvjvE"
      },
      "outputs": [],
      "source": [
        "# Run configuration with MLflow callback\n",
        "run_config = RunConfig(\n",
        "    callbacks=[mlflow_callback],\n",
        "    checkpoint_config=checkpoint_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "9EUixwZyvjvE"
      },
      "outputs": [],
      "source": [
        "# Dataset\n",
        "ds = load_data()\n",
        "train_ds, val_ds = stratify_split(ds, stratify=\"tag\", test_size=test_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "8fJYvtGxvjvE"
      },
      "outputs": [],
      "source": [
        "# Preprocess\n",
        "preprocessor = CustomPreprocessor()\n",
        "train_ds = preprocessor.fit_transform(train_ds)\n",
        "val_ds = preprocessor.transform(val_ds)\n",
        "train_ds = train_ds.materialize()\n",
        "val_ds = val_ds.materialize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "Cz4i34YQvjvF"
      },
      "outputs": [],
      "source": [
        "# Trainer\n",
        "trainer = TorchTrainer(\n",
        "    train_loop_per_worker=train_loop_per_worker,\n",
        "    train_loop_config=train_loop_config,\n",
        "    scaling_config=scaling_config,\n",
        "    run_config=run_config,  # uses RunConfig with MLflow callback\n",
        "    datasets={\"train\": train_ds, \"val\": val_ds},\n",
        "    dataset_config=dataset_config,\n",
        "    preprocessor=preprocessor,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "u0moCsCbvjvF"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Train\n",
        "results = trainer.fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "PiKYGCcuvjvF"
      },
      "outputs": [],
      "source": [
        "results.metrics_dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "gfzP4B7_vjvF"
      },
      "outputs": [],
      "source": [
        "# Sorted runs\n",
        "sorted_runs = mlflow.search_runs(experiment_names=[experiment_name], order_by=[\"metrics.val_loss ASC\"])\n",
        "sorted_runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "yAqo6S5SvjvG"
      },
      "outputs": [],
      "source": [
        "# Best run\n",
        "best_run = sorted_runs.iloc[0]\n",
        "best_run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vANFARvhvjvG"
      },
      "source": [
        "### Dashboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kme2aHazvjvG"
      },
      "source": [
        "Let's view what we've tracked from our experiment. MLFlow serves a dashboard for us to view and explore our experiments on a localhost port:\n",
        "\n",
        "```bash\n",
        "mlflow server -h 0.0.0.0 -p 8080 --backend-store-uri /tmp/mlflow/\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdqCn3Y2vjvG"
      },
      "source": [
        "MLFlow creates a main dashboard with all your experiments and their respective runs. We can sort runs by clicking on the column headers.\n",
        "\n",
        "<img src=\"https://madewithml.com/static/images/mlops/experiment_tracking/dashboard.png\" width=\"1000\" alt=\"mlflow runs\">\n",
        "\n",
        "And within each run, we can view metrics, parameters, artifacts, etc.\n",
        "\n",
        "<img src=\"https://madewithml.com/static/images/mlops/experiment_tracking/params.png\" width=\"1000\" alt=\"mlflow params\">\n",
        "\n",
        "And we can even create custom plots to help us visualize our results.\n",
        "\n",
        "<img src=\"https://madewithml.com/static/images/mlops/experiment_tracking/plots.png\" width=\"1000\" alt=\"mlflow plots\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgg9LZOrvjvG"
      },
      "source": [
        "### Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "Mom4SlALvjvG"
      },
      "outputs": [],
      "source": [
        "from ray.air import Result\n",
        "from urllib.parse import urlparse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "Lzb4StQQvjvH"
      },
      "outputs": [],
      "source": [
        "def get_best_checkpoint(run_id):\n",
        "    artifact_dir = urlparse(mlflow.get_run(run_id).info.artifact_uri).path  # get path from mlflow\n",
        "    results = Result.from_path(artifact_dir)\n",
        "    return results.best_checkpoints[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "fdIDtk0EvjvH"
      },
      "outputs": [],
      "source": [
        "# Evaluate on test split\n",
        "best_checkpoint = get_best_checkpoint(run_id=best_run.run_id)\n",
        "predictor = TorchPredictor.from_checkpoint(best_checkpoint)\n",
        "performance = evaluate(ds=test_ds, predictor=predictor)\n",
        "print (json.dumps(performance, indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "9xe3mr4EvjvH"
      },
      "outputs": [],
      "source": [
        "# Preprocessor\n",
        "preprocessor = predictor.get_preprocessor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "RDqpJjAOvjvH"
      },
      "outputs": [],
      "source": [
        "# Predict on sample\n",
        "title = \"Transfer learning with transformers\"\n",
        "description = \"Using transformers for transfer learning on text classification tasks.\"\n",
        "sample_df = pd.DataFrame([{\"title\": title, \"description\": description, \"tag\": \"other\"}])\n",
        "predict_with_proba(df=sample_df, predictor=predictor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiEQ8IegvjvH"
      },
      "source": [
        "# ⚙️&nbsp; Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "OidHEF_4vjvH"
      },
      "outputs": [],
      "source": [
        "from ray import tune\n",
        "from ray.tune import Tuner\n",
        "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
        "from ray.tune.search import ConcurrencyLimiter\n",
        "from ray.tune.search.hyperopt import HyperOptSearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osN-Z5v9vjvH"
      },
      "outputs": [],
      "source": [
        "# Number of trials (small sample)\n",
        "num_runs = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "r5ngm6WevjvI"
      },
      "outputs": [],
      "source": [
        "# Set up\n",
        "set_seeds()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "tDaex-WxvjvI"
      },
      "outputs": [],
      "source": [
        "# Dataset\n",
        "ds = load_data()\n",
        "train_ds, val_ds = stratify_split(ds, stratify=\"tag\", test_size=test_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "rsK3MBa1vjvI"
      },
      "outputs": [],
      "source": [
        "# Preprocess\n",
        "preprocessor = CustomPreprocessor()\n",
        "train_ds = preprocessor.fit_transform(train_ds)\n",
        "val_ds = preprocessor.transform(val_ds)\n",
        "train_ds = train_ds.materialize()\n",
        "val_ds = val_ds.materialize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "-9Sejw2LvjvI"
      },
      "outputs": [],
      "source": [
        "# Trainer\n",
        "trainer = TorchTrainer(\n",
        "    train_loop_per_worker=train_loop_per_worker,\n",
        "    train_loop_config=train_loop_config,\n",
        "    scaling_config=scaling_config,\n",
        "    datasets={\"train\": train_ds, \"val\": val_ds},\n",
        "    dataset_config=dataset_config,\n",
        "    preprocessor=preprocessor,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "SXVYEvrGvjvI"
      },
      "outputs": [],
      "source": [
        "# MLflow callback\n",
        "mlflow_callback = MLflowLoggerCallback(\n",
        "    tracking_uri=MLFLOW_TRACKING_URI,\n",
        "    experiment_name=experiment_name,\n",
        "    save_artifact=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "lCP5YZ7YvjvI"
      },
      "outputs": [],
      "source": [
        "# Run configuration\n",
        "checkpoint_config = CheckpointConfig(num_to_keep=1, checkpoint_score_attribute=\"val_loss\", checkpoint_score_order=\"min\")\n",
        "run_config = RunConfig(\n",
        "    callbacks=[mlflow_callback],\n",
        "    checkpoint_config=checkpoint_config\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "Rrq2XrxqvjvI"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters to start with\n",
        "initial_params = [{\"train_loop_config\": {\"dropout_p\": 0.5, \"lr\": 1e-4, \"lr_factor\": 0.8, \"lr_patience\": 3}}]\n",
        "search_alg = HyperOptSearch(points_to_evaluate=initial_params)\n",
        "search_alg = ConcurrencyLimiter(search_alg, max_concurrent=2)  # trade off b/w optimization and search space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "PFTLg0T6vjvI"
      },
      "outputs": [],
      "source": [
        "# Parameter space\n",
        "param_space = {\n",
        "    \"train_loop_config\": {\n",
        "        \"dropout_p\": tune.uniform(0.3, 0.9),\n",
        "        \"lr\": tune.loguniform(1e-5, 5e-4),\n",
        "        \"lr_factor\": tune.uniform(0.1, 0.9),\n",
        "        \"lr_patience\": tune.uniform(1, 10),\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "RyxuqQswvjvI"
      },
      "outputs": [],
      "source": [
        "# Scheduler\n",
        "scheduler = AsyncHyperBandScheduler(\n",
        "    max_t=train_loop_config[\"num_epochs\"],  # max epoch (<time_attr>) per trial\n",
        "    grace_period=5,  # min epoch (<time_attr>) per trial\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "AVIfIarUvjvJ"
      },
      "outputs": [],
      "source": [
        "# Tune config\n",
        "tune_config = tune.TuneConfig(\n",
        "    metric=\"val_loss\",\n",
        "    mode=\"min\",\n",
        "    search_alg=search_alg,\n",
        "    scheduler=scheduler,\n",
        "    num_samples=num_runs,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "OA1FE5n8vjvJ"
      },
      "outputs": [],
      "source": [
        "# Tuner\n",
        "tuner = Tuner(\n",
        "    trainable=trainer,\n",
        "    run_config=run_config,\n",
        "    param_space=param_space,\n",
        "    tune_config=tune_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "TjRUL-YpvjvJ"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Tune\n",
        "results = tuner.fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "716TGoPFvjvJ"
      },
      "outputs": [],
      "source": [
        "# All trials in experiment\n",
        "results.get_dataframe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "4wW9zTYQvjvJ"
      },
      "outputs": [],
      "source": [
        "# Best trial's epochs\n",
        "best_trial = results.get_best_result(metric=\"val_loss\", mode=\"min\")\n",
        "best_trial.metrics_dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "jCIgLBghvjvJ"
      },
      "outputs": [],
      "source": [
        "# Best trial's hyperparameters\n",
        "best_trial.config[\"train_loop_config\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "ZaAkOo-SvjvJ"
      },
      "outputs": [],
      "source": [
        "# Sorted runs\n",
        "sorted_runs = mlflow.search_runs(experiment_names=[experiment_name], order_by=[\"metrics.val_loss ASC\"])\n",
        "sorted_runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "2kbba_YevjvJ"
      },
      "outputs": [],
      "source": [
        "# Evaluate on test split\n",
        "run_id = sorted_runs.iloc[0].run_id\n",
        "best_checkpoint = get_best_checkpoint(run_id=run_id)\n",
        "predictor = TorchPredictor.from_checkpoint(best_checkpoint)\n",
        "performance = evaluate(ds=test_ds, predictor=predictor)\n",
        "print (json.dumps(performance, indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "rBIaUtkyvjvK"
      },
      "outputs": [],
      "source": [
        "# Preprocessor\n",
        "preprocessor = predictor.get_preprocessor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "icE2rHXGvjvK"
      },
      "outputs": [],
      "source": [
        "# Predict on sample\n",
        "title = \"Transfer learning with transformers\"\n",
        "description = \"Using transformers for transfer learning on text classification tasks.\"\n",
        "sample_df = pd.DataFrame([{\"title\": title, \"description\": description, \"tag\": \"other\"}])\n",
        "predict_with_proba(df=sample_df, predictor=predictor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ofaM94omwgY"
      },
      "source": [
        "# ⚖️&nbsp; Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsj8_EUEmynv"
      },
      "source": [
        "So far we've been evaluating our models by determing the overall precision, recall and f1 scores. But since performance is one of the key decision making factors when comparing different models, we should have even more nuanced evaluation strategies.\n",
        "\n",
        "- Coarse-grained metrics\n",
        "- Fine-grained metrics\n",
        "- Confusion matrix\n",
        "- Confidence learning\n",
        "- Slice metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h51AAn1Fu4b5",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Metrics\n",
        "metrics = {\"overall\": {}, \"class\": {}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8BgzzHBZNMn",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# y_test\n",
        "preprocessor = predictor.get_preprocessor()\n",
        "preprocessed_ds = preprocessor.transform(test_ds)\n",
        "values = preprocessed_ds.select_columns(cols=[\"targets\"]).take_all()\n",
        "y_test = np.stack([item[\"targets\"] for item in values])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "_P8rIe2IvjvK"
      },
      "outputs": [],
      "source": [
        "# y_pred\n",
        "test_df = test_ds.to_pandas()\n",
        "z = predictor.predict(data=test_df)[\"predictions\"]  # adds text column (in-memory)\n",
        "y_pred = np.stack(z).argmax(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "p994y2HWvjvK"
      },
      "outputs": [],
      "source": [
        "# y_prob\n",
        "y_prob = torch.tensor(np.stack(z)).softmax(dim=1).numpy()\n",
        "print (np.shape(y_test))\n",
        "print (np.shape(y_prob))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "6A9qQt5fvjvL"
      },
      "outputs": [],
      "source": [
        "# Add columns (for convenience)\n",
        "test_df = test_ds.to_pandas()\n",
        "test_df[\"text\"] = test_df[\"title\"] + \" \" + test_df[\"description\"]\n",
        "test_df[\"prediction\"] = test_df.index.map(lambda i: preprocessor.index_to_class[y_pred[i]])\n",
        "test_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiXcls5JoNA8"
      },
      "source": [
        "### Coarse-grained metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2OQtNODrh6c",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Overall metrics\n",
        "overall_metrics = precision_recall_fscore_support(y_test, y_pred, average=\"weighted\")\n",
        "metrics[\"overall\"][\"precision\"] = overall_metrics[0]\n",
        "metrics[\"overall\"][\"recall\"] = overall_metrics[1]\n",
        "metrics[\"overall\"][\"f1\"] = overall_metrics[2]\n",
        "metrics[\"overall\"][\"num_samples\"] = np.float64(len(y_test))\n",
        "print (json.dumps(metrics[\"overall\"], indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zl3xSuXRutKG"
      },
      "source": [
        "### Fine-grained metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqetm3ybN9C1",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zIAI4mwusoX",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Per-class metrics\n",
        "class_metrics = precision_recall_fscore_support(y_test, y_pred, average=None)\n",
        "for i, _class in enumerate(preprocessor.class_to_index):\n",
        "    metrics[\"class\"][_class] = {\n",
        "        \"precision\": class_metrics[0][i],\n",
        "        \"recall\": class_metrics[1][i],\n",
        "        \"f1\": class_metrics[2][i],\n",
        "        \"num_samples\": np.float64(class_metrics[3][i]),\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rhh-tgpP0dvj",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Metrics for a specific class\n",
        "tag = \"natural-language-processing\"\n",
        "print (json.dumps(metrics[\"class\"][tag], indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQVA6G-j__t5",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Sorted tags\n",
        "sorted_tags_by_f1 = OrderedDict(sorted(\n",
        "        metrics[\"class\"].items(), key=lambda tag: tag[1][\"f1\"], reverse=True))\n",
        "for item in sorted_tags_by_f1.items():\n",
        "    print (json.dumps(item, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-juex26zvBF"
      },
      "source": [
        "### Confusion matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPUao0S4k99c"
      },
      "source": [
        "- **True positives (TP)**: learn about where our model performs well.\n",
        "- **False positives (FP)**: potentially identify samples which may need to be relabeled.\n",
        "- False negatives (FN): identify the model's less performant areas to oversample later.\n",
        "\n",
        "> It's a good to have our FP/FN samples feed back into our annotation pipelines in the event we want to fix their labels and have those changes be reflected everywhere."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZG2SgsPAzukL",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# TP, FP, FN samples\n",
        "tag = \"natural-language-processing\"\n",
        "index = preprocessor.class_to_index[tag]\n",
        "tp, fp, fn = [], [], []\n",
        "for i, true in enumerate(y_test):\n",
        "    pred = y_pred[i]\n",
        "    if index==true==pred:\n",
        "        tp.append(i)\n",
        "    elif index!=true and index==pred:\n",
        "        fp.append(i)\n",
        "    elif index==true and index!=pred:\n",
        "        fn.append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePrxeVkG0mmO",
        "tags": []
      },
      "outputs": [],
      "source": [
        "print (tp)\n",
        "print (fp)\n",
        "print (fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5IWiMyRvjvO"
      },
      "outputs": [],
      "source": [
        "# Samples\n",
        "num_samples = 3\n",
        "cm = [(tp, \"True positives\"), (fp, \"False positives\"), (fn, \"False negatives\")]\n",
        "for item in cm:\n",
        "    if len(item[0]):\n",
        "        print (f\"\\n=== {item[1]} ===\")\n",
        "        for index in item[0][:num_samples]:\n",
        "            print (f\"{test_df.iloc[index].text}\")\n",
        "            print (f\"    true: {test_df.tag[index]}\")\n",
        "            print (f\"    pred: {test_df.prediction[index]}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6S5LZdP2Myjh"
      },
      "source": [
        "### Confidence learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZW5nY_h-M08p"
      },
      "source": [
        "While the confusion-matrix sample analysis was a coarse-grained process, we can also use fine-grained confidence based approaches to identify potentially mislabeled samples. Here we’re going to focus on the specific labeling quality as opposed to the final model predictions.\n",
        "\n",
        "Simple confidence based techniques include identifying samples whose:\n",
        "\n",
        "**Categorical**\n",
        "- prediction is incorrect (also indicate TN, FP, FN)\n",
        "- confidence score for the correct class is below a threshold\n",
        "- confidence score for an incorrect class is above a threshold\n",
        "- standard deviation of confidence scores over top N samples is low\n",
        "- different predictions from same model using different parameters\n",
        "\n",
        "**Continuous**\n",
        "- difference between predicted and ground-truth values is above some %\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuN8xKFZlo2t"
      },
      "source": [
        "> The operations in this section can be applied to entire labeled dataset to discover labeling errors via confidence learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FCrRUb2GANr",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Tag to inspect\n",
        "tag = \"natural-language-processing\"\n",
        "index = class_to_index[tag]\n",
        "indices = np.where(y_test==index)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKQxFU0iU-w-",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Confidence score for the correct class is below a threshold\n",
        "low_confidence = []\n",
        "min_threshold = 0.5\n",
        "for i in indices:\n",
        "    prob = y_prob[i][index]\n",
        "    if prob <= 0.5:\n",
        "        low_confidence.append({\n",
        "            \"text\": f\"{test_df.iloc[i].text}\",\n",
        "            \"true\": test_df.tag[i],\n",
        "            \"pred\": test_df.prediction[i],\n",
        "            \"prob\": prob})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DnkXhXFFMv_",
        "tags": []
      },
      "outputs": [],
      "source": [
        "low_confidence[0:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwL1ltdiUjH2"
      },
      "source": [
        "But these are fairly crude techniques because neural networks are easily [overconfident](https://arxiv.org/abs/1706.04599) and so their confidences cannot be used without calibrating them.\n",
        "\n",
        "<div class=\"ai-center-all\">\n",
        "    <img src=\"https://madewithml.com/static/images/mlops/evaluation/calibration.png\" width=\"300\" alt=\"accuracy vs. confidence\">\n",
        "</div>\n",
        "<div class=\"ai-center-all mt-1\">\n",
        "  <small>Modern (large) neural networks result in higher accuracies but are over confident.<br><a href=\"https://arxiv.org/abs/1706.04599\" target=\"_blank\">On Calibration of Modern Neural Networks</a></small>\n",
        "</div>\n",
        "\n",
        "* **Assumption**: *“the probability associated with the predicted class label should reflect its ground truth correctness likelihood.”*\n",
        "* **Reality**: *“modern (large) neural networks are no longer well-calibrated”*\n",
        "* **Solution**: apply temperature scaling (extension of [Platt scaling](https://en.wikipedia.org/wiki/Platt_scaling){:target=\"_blank\"}) on model outputs\n",
        "\n",
        "Recent work on [confident learning](https://arxiv.org/abs/1911.00068) focuses on identifying noisy labels while accounting for this overconfidence which can then be properly relabeled and used for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XX3cORGPPXXM",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import cleanlab\n",
        "from cleanlab.filter import find_label_issues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "KTIoKGSuvjvQ"
      },
      "outputs": [],
      "source": [
        "# Find label issues\n",
        "label_issues = find_label_issues(labels=y_test, pred_probs=y_prob, return_indices_ranked_by=\"self_confidence\")\n",
        "test_df.iloc[label_issues].drop(columns=[\"text\"]).head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtXjpKf9FU4C"
      },
      "source": [
        "Not all of these are necessarily labeling errors but situations where the predicted probabilities were not so confident. Therefore, it will be useful to attach the predictions alongside the data. This way, we can know if we need to relabel, upsample, etc. to improve our performance. Analysis like this could also shed light on the task itself. For example, you may notice that some projects involve multiple data modalities and so it's difficult to just assing one tag. So perhaps it might be better to make this taks a multilabel classification task instead (it does but we simplified it for this course)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvS3UpusXP_R"
      },
      "source": [
        "### Slice metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeWWMG38Ny4U"
      },
      "source": [
        "Just inspecting the overall and class metrics isn't enough to deploy our new version to production. There may be key slices of our dataset that we need to do really well on:\n",
        "\n",
        "- Target / predicted classes (+ combinations)\n",
        "- Features (explicit and implicit)\n",
        "- Metadata (timestamps, sources, etc.)\n",
        "- Priority slices / experience (minority groups, large customers, etc.)\n",
        "\n",
        "An easy way to create and evaluate slices is to define slicing functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyueOtQsXdGm",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from snorkel.slicing import PandasSFApplier\n",
        "from snorkel.slicing import slice_dataframe\n",
        "from snorkel.slicing import slicing_function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coutP2KtXdLG",
        "tags": []
      },
      "outputs": [],
      "source": [
        "@slicing_function()\n",
        "def nlp_llm(x):\n",
        "    \"\"\"NLP projects that use LLMs.\"\"\"\n",
        "    nlp_project = \"natural-language-processing\" in x.tag\n",
        "    llm_terms = [\"transformer\", \"llm\", \"bert\"]\n",
        "    llm_project = any(s.lower() in x.text.lower() for s in llm_terms)\n",
        "    return (nlp_project and llm_project)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbxmLvi-D7lq",
        "tags": []
      },
      "outputs": [],
      "source": [
        "@slicing_function()\n",
        "def short_text(x):\n",
        "    \"\"\"Projects with short titles and descriptions.\"\"\"\n",
        "    return len(x.text.split()) < 8  # less than 8 words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Vxg5X9OD-Ax"
      },
      "source": [
        "Here we're using Snorkel's [`slicing_function`](https://snorkel.readthedocs.io/en/latest/packages/_autosummary/slicing/snorkel.slicing.slicing_function.html) to create our different slices. We can visualize our slices by applying this slicing function to a relevant DataFrame using [`slice_dataframe`](https://snorkel.readthedocs.io/en/latest/packages/_autosummary/slicing/snorkel.slicing.slice_dataframe.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRs93KeBMthW",
        "tags": []
      },
      "outputs": [],
      "source": [
        "nlp_llm_df = slice_dataframe(test_df, nlp_llm)\n",
        "nlp_llm_df[[\"text\", \"tag\"]].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7jmdmNaXuA2",
        "tags": []
      },
      "outputs": [],
      "source": [
        "short_text_df = slice_dataframe(test_df, short_text)\n",
        "short_text_df[[\"text\", \"tag\"]].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZuDZwTNO93Q"
      },
      "source": [
        "We can define even more slicing functions and create a slices record array using the [`PandasSFApplier`](https://snorkel.readthedocs.io/en/latest/packages/_autosummary/slicing/snorkel.slicing.PandasSFApplier.html). The slices array has N (# of data points) items and each item has S (# of slicing functions) items, indicating whether that data point is part of that slice. Think of this record array as a masking layer for each slicing function on our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQG8PFovXfEm",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Slices\n",
        "slicing_functions = [nlp_llm, short_text]\n",
        "applier = PandasSFApplier(slicing_functions)\n",
        "slices = applier.apply(test_df)\n",
        "slices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAWPU-f-GIOD"
      },
      "source": [
        "To calculate metrics for our slices, we could use [snorkel.analysis.Scorer](https://snorkel.readthedocs.io/en/latest/packages/_autosummary/analysis/snorkel.analysis.Scorer.html) but we've implemented a version that will work for multiclass or multilabel scenarios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqkwQenBXfIa",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Score slices\n",
        "metrics[\"slices\"] = {}\n",
        "for slice_name in slices.dtype.names:\n",
        "    mask = slices[slice_name].astype(bool)\n",
        "    if sum(mask):\n",
        "        slice_metrics = precision_recall_fscore_support(\n",
        "            y_test[mask], y_pred[mask], average=\"micro\"\n",
        "        )\n",
        "        metrics[\"slices\"][slice_name] = {}\n",
        "        metrics[\"slices\"][slice_name][\"precision\"] = slice_metrics[0]\n",
        "        metrics[\"slices\"][slice_name][\"recall\"] = slice_metrics[1]\n",
        "        metrics[\"slices\"][slice_name][\"f1\"] = slice_metrics[2]\n",
        "        metrics[\"slices\"][slice_name][\"num_samples\"] = len(y_test[mask])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QapvZ3bgX3J6",
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(json.dumps(metrics[\"slices\"], indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmEOEHFEMyI1"
      },
      "source": [
        "Slicing can help identify sources of *bias* in our data. For example, our model has most likely learned to associated algorithms with certain applications such as CNNs used for computer vision or transformers used for NLP projects. However, these algorithms are not being applied beyond their initial use cases. We’d need ensure that our model learns to focus on the application over algorithm. This could be learned with:\n",
        "\n",
        "- enough data (new or oversampling incorrect predictions)\n",
        "- masking the algorithm (using text matching heuristics)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuCLIa2c9YEY"
      },
      "source": [
        "### Interpretability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4v0CxdbqLCvd"
      },
      "source": [
        "Besides just comparing predicted outputs with ground truth values, we can also inspect the inputs to our models. What aspects of the input are more influential towards the prediction? If the focus is not on the relevant features of our input, then we need to explore if there is a hidden pattern we're missing or if our model has learned to overfit on the incorrect features. We can use techniques such as [SHAP](https://github.com/slundberg/shap) (SHapley Additive exPlanations) or [LIME](https://github.com/marcotcr/lime) (Local Interpretable Model-agnostic Explanations) to inspect feature importance. On a high level, these techniques learn which features have the most signal by assessing the performance in their absence. These inspections can be performed on a global level (ex. per-class) or on a local level (ex. single prediction)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aW6CPXnPC61M",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from lime.lime_text import LimeTextExplainer\n",
        "from sklearn.pipeline import make_pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndrizK-DLRSA"
      },
      "source": [
        "[`LimeTextExplainer.explain_instance`](https://lime-ml.readthedocs.io/en/latest/lime.html#lime.lime_text.LimeTextExplainer.explain_instance) function requires a `classifier_fn` that takes in a list of strings and outputs the predicted probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dYPTovdL6QX",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def classifier_fn(texts):\n",
        "    df = pd.DataFrame({\"title\": texts, \"description\": \"\", \"tag\": \"other\"})\n",
        "    z = predictor.predict(data=df)[\"predictions\"]\n",
        "    y_prob = torch.tensor(np.stack(z)).softmax(dim=1).numpy()\n",
        "    return y_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1tR1lyJC68X",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Explain instance\n",
        "text = \"Using pretrained convolutional neural networks for object detection.\"\n",
        "explainer = LimeTextExplainer(class_names=list(class_to_index.keys()))\n",
        "explainer.explain_instance(text, classifier_fn=classifier_fn, top_labels=1).show_in_notebook(text=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Pkm_pH847x1"
      },
      "source": [
        "### Behavioral testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUsd9Z8347x1",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# INVariance via verb injection (changes should not affect outputs)\n",
        "tokens = [\"revolutionized\", \"disrupted\"]\n",
        "texts = [f\"Transformers applied to NLP have {token} the ML field.\" for token in tokens]\n",
        "[preprocessor.index_to_class[y_prob.argmax()] for y_prob in classifier_fn(texts=texts)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VLqZDYr47x2",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# DIRectional expectations (changes with known outputs)\n",
        "tokens = [\"text classification\", \"image classification\"]\n",
        "texts = [f\"ML applied to {token}.\" for token in tokens]\n",
        "[preprocessor.index_to_class[y_prob.argmax()] for y_prob in classifier_fn(texts=texts)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OW57njXQ47x2",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Minimum Functionality Tests (simple input/output pairs)\n",
        "tokens = [\"natural language processing\", \"mlops\"]\n",
        "texts = [f\"{token} is the next big wave in machine learning.\" for token in tokens]\n",
        "[preprocessor.index_to_class[y_prob.argmax()] for y_prob in classifier_fn(texts=texts)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkBxFVAA47x2"
      },
      "source": [
        "We'll learn how to systematically create tests in our [testing lesson](https://madewithml.com/courses/mlops/testing#behavioral-testing). Be sure to also checkout the [evaluation lesson](https://madewithml.com/courses/mlops/evaluation) where we cover more ways to evaluate our model, including generating slices, counterfactuals and more."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QguIkqeOvjvV"
      },
      "source": [
        "# 🚀&nbsp; Serving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "8oHyiJgHvjvV"
      },
      "source": [
        "### Batch inference (offline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "aG9jV-mDvjvW"
      },
      "outputs": [],
      "source": [
        "import ray.data\n",
        "from ray.train.torch import TorchPredictor\n",
        "from ray.data import ActorPoolStrategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "4J2FkfF0vjvW"
      },
      "outputs": [],
      "source": [
        "# Load predictor\n",
        "run_id = sorted_runs.iloc[0].run_id\n",
        "best_checkpoint = get_best_checkpoint(run_id=run_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "w-7JC7LLvjvW"
      },
      "outputs": [],
      "source": [
        "class Predictor:\n",
        "    def __init__(self, checkpoint):\n",
        "        self.predictor = TorchPredictor.from_checkpoint(checkpoint)\n",
        "    def __call__(self, batch):\n",
        "        z = self.predictor.predict(batch)[\"predictions\"]\n",
        "        y_pred = np.stack(z).argmax(1)\n",
        "        prediction = decode(y_pred, preprocessor.index_to_class)\n",
        "        return {\"prediction\": prediction}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "oT3sIivzvjvW"
      },
      "outputs": [],
      "source": [
        "# Batch predict\n",
        "predictions = test_ds.map_batches(\n",
        "    Predictor,\n",
        "    batch_size=128,\n",
        "    compute=ActorPoolStrategy(min_size=1, max_size=2),\n",
        "    batch_format=\"pandas\",\n",
        "    fn_constructor_kwargs={\"checkpoint\": best_checkpoint})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vr0L0yvRvjvW"
      },
      "outputs": [],
      "source": [
        "# Sample predictions\n",
        "predictions.take(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PD-ommouvjvW"
      },
      "source": [
        "### Online inference (real-time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6EPxbzPvjvX"
      },
      "source": [
        "While we can achieve batch inference at scale, many models will need to be served in an real-time manner where we may need to deliver predictions for many incoming requests (high throughput) with low latency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "NYcOk3qJvjvX"
      },
      "outputs": [],
      "source": [
        "from fastapi import FastAPI\n",
        "from ray import serve\n",
        "import requests\n",
        "from starlette.requests import Request"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "Kx6epP08vjvX"
      },
      "outputs": [],
      "source": [
        "# Define application\n",
        "app = FastAPI(\n",
        "    title=\"Made With ML\",\n",
        "    description=\"Classify machine learning projects.\",\n",
        "    version=\"0.1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKLlLqTzvjvX"
      },
      "source": [
        "We'll start by defining our FastAPI application which involves initializing a predictor (and preprocessor) from the best checkpoint for a particular run (specified by `run_id`). We'll also define a `predict` function that will be used to make predictions on our input data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "N0dIlM2WvjvX"
      },
      "outputs": [],
      "source": [
        "@serve.deployment(route_prefix=\"/\", num_replicas=\"1\", ray_actor_options={\"num_cpus\": 8, \"num_gpus\": 0})\n",
        "@serve.ingress(app)\n",
        "class ModelDeployment:\n",
        "\n",
        "    def __init__(self, run_id):\n",
        "        \"\"\"Initialize the model.\"\"\"\n",
        "        self.run_id = run_id\n",
        "        mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)  # so workers have access to model registry\n",
        "        best_checkpoint = get_best_checkpoint(run_id=run_id)\n",
        "        self.predictor = TorchPredictor.from_checkpoint(best_checkpoint)\n",
        "        self.preprocessor = self.predictor.get_preprocessor()\n",
        "\n",
        "    @app.post(\"/predict/\")\n",
        "    async def _predict(self, request: Request):\n",
        "        data = await request.json()\n",
        "        df = pd.DataFrame([{\"title\": data.get(\"title\", \"\"), \"description\": data.get(\"description\", \"\"), \"tag\": \"\"}])\n",
        "        results = predict_with_proba(df=df, predictor=self.predictor)\n",
        "        return {\"results\": results}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUIgd6X1vjvX"
      },
      "source": [
        "> `async def` refers to an asynchronous function (when we call the function we don't have to wait for the function to complete executing). The `await` keyword is used inside an asynchronous function to wait for the completion of the `request.json()` operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "LISqg8kAvjvY"
      },
      "outputs": [],
      "source": [
        "# Run service\n",
        "sorted_runs = mlflow.search_runs(experiment_names=[experiment_name], order_by=[\"metrics.val_loss ASC\"])\n",
        "run_id = sorted_runs.iloc[0].run_id\n",
        "serve.run(ModelDeployment.bind(run_id=run_id))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "5OFen1bSvjvY"
      },
      "outputs": [],
      "source": [
        "# Query\n",
        "title = \"Transfer learning with transformers\"\n",
        "description = \"Using transformers for transfer learning on text classification tasks.\"\n",
        "json_data = json.dumps({\"title\": title, \"description\": description})\n",
        "requests.post(\"http://127.0.0.1:8000/predict/\", data=json_data).json()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oz3h69aVvjvY"
      },
      "source": [
        "The issue with neural networks (and especially LLMs) is that they are notoriously overconfident. For every input, they will always make some prediction. And to account for this, we have an `other` class but that class only has projects that are not in our accepted tags but are still machine learning related nonetheless. Here's what happens when we input complete noise as our input:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "FqxOqP7UvjvY"
      },
      "outputs": [],
      "source": [
        "# Query (noise)\n",
        "title = \" 65n7r5675\"  # random noise\n",
        "json_data = json.dumps({\"title\": title, \"description\": \"\"})\n",
        "requests.post(\"http://127.0.0.1:8000/predict/\", data=json_data).json()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "uFg6uLKyvjvY"
      },
      "outputs": [],
      "source": [
        "# Shutdown\n",
        "serve.shutdown()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOTC6l4GvjvZ"
      },
      "source": [
        "### Custom logic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzwy-x3RvjvZ"
      },
      "source": [
        "To make our service a bit more robust, let's add some custom logic to predict the `other` class if the probability of the predicted class is below a certain `threshold` probability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "EaUwbQBHvjvZ"
      },
      "outputs": [],
      "source": [
        "@serve.deployment(route_prefix=\"/\", num_replicas=\"1\", ray_actor_options={\"num_cpus\": 8, \"num_gpus\": 0})\n",
        "@serve.ingress(app)\n",
        "class ModelDeploymentRobust:\n",
        "\n",
        "    def __init__(self, run_id, threshold=0.9):\n",
        "        \"\"\"Initialize the model.\"\"\"\n",
        "        self.run_id = run_id\n",
        "        self.threshold = threshold\n",
        "        mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)  # so workers have access to model registry\n",
        "        best_checkpoint = get_best_checkpoint(run_id=run_id)\n",
        "        self.predictor = TorchPredictor.from_checkpoint(best_checkpoint)\n",
        "        self.preprocessor = self.predictor.get_preprocessor()\n",
        "\n",
        "    @app.post(\"/predict/\")\n",
        "    async def _predict(self, request: Request):\n",
        "        data = await request.json()\n",
        "        df = pd.DataFrame([{\"title\": data.get(\"title\", \"\"), \"description\": data.get(\"description\", \"\"), \"tag\": \"\"}])\n",
        "        results = predict_with_proba(df=df, predictor=self.predictor)\n",
        "\n",
        "        # Apply custom logic\n",
        "        for i, result in enumerate(results):\n",
        "            pred = result[\"prediction\"]\n",
        "            prob = result[\"probabilities\"]\n",
        "            if prob[pred] < self.threshold:\n",
        "                results[i][\"prediction\"] = \"other\"\n",
        "\n",
        "        return {\"results\": results}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "XawyQEO9vjvZ"
      },
      "outputs": [],
      "source": [
        "# Run service\n",
        "serve.run(ModelDeploymentRobust.bind(run_id=run_id, threshold=0.9))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "kXF7kq3Mvjve"
      },
      "outputs": [],
      "source": [
        "# Query (noise)\n",
        "title = \" 65n7r5675\"  # random noise\n",
        "json_data = json.dumps({\"title\": title, \"description\": \"\"})\n",
        "requests.post(\"http://127.0.0.1:8000/predict/\", data=json_data).json()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "cUCnOt8kvjve"
      },
      "outputs": [],
      "source": [
        "# Shutdown\n",
        "serve.shutdown()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fYlG-q6vjve"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRLNj0C747x_",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Clean up\n",
        "!rm -rf ~/ray_results\n",
        "!rm -rf $MODEL_REGISTRY"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "toc-autonumbering": false,
    "toc-showcode": false,
    "toc-showmarkdowntxt": false,
    "toc-showtags": true,
    "vscode": {
      "interpreter": {
        "hash": "8071f8c6175eca3e18fb18842fbea041b655e67ca9cf317ca1066c8b060a000c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}